{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_union\n",
    "from scipy.sparse import hstack\n",
    "from sklearn import metrics\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import warnings\n",
    "import joblib # for deserialization saved models \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlserving import ServingApp\n",
    "from mlserving.predictors import RESTPredictor\n",
    "\n",
    "import joblib # for deserialization saved models \n",
    "\n",
    "\n",
    "class MyPredictor(RESTPredictor):\n",
    "    def __init__(self):\n",
    "        # Loading a saved model\n",
    "        self.gloveModel = joblib.load('gloveModel.pkl')\n",
    "        self.vectorizer = joblib.load('vectorizer.pkl')\n",
    "        self.feature_names = self.vectorizer.get_feature_names()\n",
    "        self.lr_insult = joblib.load('lr_insult.pkl')\n",
    "        self.lr_obscene = joblib.load('lr_obscene.pkl')\n",
    "        self.lr_threat = joblib.load('lr_threat.pkl')\n",
    "        self.lr_toxic = joblib.load('lr_toxic.pkl')\n",
    "        self.lr_identity_hate = joblib.load('lr_identity_hate.pkl')\n",
    "\n",
    "    def get_word_weight(self, text):\n",
    "        \"\"\"Returns a dictionary where keys are the words of the text and values are their weights.\"\"\"\n",
    "        tfidf_matrix = self.vectorizer.transform([text]).todense()\n",
    "        feature_index = tfidf_matrix[0,:].nonzero()[1]\n",
    "        tfidf_scores = zip([self.feature_names[i] for i in feature_index], [tfidf_matrix[0, x] for x in feature_index])\n",
    "        return dict(tfidf_scores)\n",
    "        \n",
    "    def num_upper(self, text):\n",
    "        \"\"\"Returns the number of capital letters in a string.\"\"\"\n",
    "        num = 0\n",
    "        for i in text:\n",
    "            if i.isupper():\n",
    "                num += 1\n",
    "        return num\n",
    "    \n",
    "    def weighted_vector_mean(self, text):\n",
    "        \"\"\"Gets the weighted vector mean of a sentence by averaging the word vectors according to Tfidf weights.\"\"\"\n",
    "        sentence_vects = []\n",
    "        sentence_weights = []\n",
    "        words = text.split(\" \")\n",
    "        words = [word for word in words if word in self.gloveModel.wv.vocab]\n",
    "\n",
    "        text_dict = self.get_word_weight(text)\n",
    "        total = sum(text_dict.values())\n",
    "        text_dict = {key:(val/total) for key,val in text_dict.items()}\n",
    "\n",
    "        for word in words:\n",
    "            sentence_vects.append(self.gloveModel[word])               # get word vectors\n",
    "            if word.lower() in text_dict.keys():\n",
    "                sentence_weights.append(text_dict[word.lower()])   # get weights of words\n",
    "            else:\n",
    "                sentence_weights.append(0)\n",
    "\n",
    "        if len(sentence_vects) > 0:\n",
    "            return np.transpose(sentence_vects) @ sentence_weights / len(sentence_vects)\n",
    "        else:\n",
    "            return np.zeros(300)\n",
    "\n",
    "    def create_df(self, text):\n",
    "        txt = text\n",
    "        d = {'text': [txt]}\n",
    "        df = pd.DataFrame(data=d)\n",
    "        return df\n",
    "    \n",
    "    def generate_features(self, df):\n",
    "        dfc = df.copy()\n",
    "        # Cleaning text\n",
    "        dfc['text'] = dfc['text'].str.replace(r\"[(\\.),(\\|)!:='&(\\*)(\\\")]\", \"\")\n",
    "        dfc['text'] = dfc['text'].str.replace(\"\\n\", \"\")\n",
    "\n",
    "        # Getting length\n",
    "        dfc['len'] = dfc['text'].apply(len) - dfc['text'].str.count(\" \")\n",
    "        len_min = 0\n",
    "        len_max = 127\n",
    "        dfc['len'] = (dfc['len'].values - len_min) / (len_max - len_min)\n",
    "\n",
    "        # Getting proportion of caps\n",
    "        dfc['caps'] = dfc['text'].apply(self.num_upper)\n",
    "        dfc['proportion of caps'] = dfc['caps'] / dfc['len']\n",
    "\n",
    "        # Accounting for division by 0\n",
    "        dfc['proportion of caps'] = dfc['proportion of caps'].fillna(0)\n",
    "\n",
    "        # Adding the 300D vector means, weighted by Tfidf weights\n",
    "        dfc['vector mean'] = dfc['text'].apply(self.weighted_vector_mean)\n",
    "        tmp = pd.DataFrame(dfc['vector mean'].tolist())\n",
    "        dfc = dfc.join(tmp)\n",
    "        dfc = dfc.drop(['vector mean', 'text', 'caps'], axis=1)\n",
    "        return dfc\n",
    "    \n",
    "    def pre_process(self, input_data, req):\n",
    "        text = input_data['features']\n",
    "        df_text = self.create_df(text)\n",
    "        dfc_text = self.generate_features(df_text)\n",
    "        t_text = df_text['text']\n",
    "        t_vector = self.vectorizer.transform(t_text)\n",
    "        final_testing = hstack([t_vector, dfc_text[['len', 'proportion of caps']]])\n",
    "        return final_testing\n",
    "#         return input_data['features']\n",
    "\n",
    "    def predict(self, processed_data, req):\n",
    "        print(type(processed_data))\n",
    "        print(processed_data.shape)\n",
    "        # print(self.lr_obscene.predict(final_testing))\n",
    "        # print(self.lr_toxic.predict(final_testing))\n",
    "        # print(self.lr_identity_hate.predict(final_testing))\n",
    "        # print(self.lr_threat.predict(final_testing))\n",
    "        # print(self.lr_insult.predict(final_testing))\n",
    "        return self.model.polarity_scores(processed_data)\n",
    "\n",
    "    def post_process(self, prediction, req):\n",
    "        return {'results': prediction}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\dheeraj\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\base.py:334: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.22.1 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "c:\\users\\dheeraj\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\base.py:334: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.22.1 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "c:\\users\\dheeraj\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\base.py:334: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.22.1 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-11-30 20:19:40,904] - INFO - Running development server on: http://0.0.0.0:5000/\n",
      "[2020-11-30 20:19:40,909] - WARNING - NOTICE! Running development server on production environment is not recommended.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [30/Nov/2020 21:11:35] \"OPTIONS /api/v1/predict HTTP/1.1\" 200 0\n",
      "c:\\users\\dheeraj\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:39: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "127.0.0.1 - - [30/Nov/2020 21:11:35] \"POST /api/v1/predict HTTP/1.1\" 500 515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "(1, 357956)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\dheeraj\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:39: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "127.0.0.1 - - [30/Nov/2020 21:11:36] \"POST /api/v1/predict HTTP/1.1\" 500 515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "(1, 357956)\n"
     ]
    }
   ],
   "source": [
    "app = ServingApp()\n",
    "app.add_inference_handler('/api/v1/predict', MyPredictor())\n",
    "app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
